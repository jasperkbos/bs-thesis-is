{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7646990",
   "metadata": {},
   "source": [
    "# Preprocessing the Dutch RTE-3 corpus\n",
    "This script preprocesses RTE-3 and serialises the result to the dev, train and test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99ee20",
   "metadata": {},
   "source": [
    "## Inspecting the RTE-3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea3a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_distribution(rte3_txt, split):\n",
    "    \"\"\"\n",
    "    Retrieves the distribution of the RTE-3 txt.\n",
    "\n",
    "    :param rte3_txt: path to the translated RTE-3 txt\n",
    "    :param split: denotes which split to retrieve the distribution of, either \"dev\" or \"test\"\n",
    "    :return: dict denoting the distribution of the entailment and length label divided over the four NLP tasks\n",
    "    \"\"\"\n",
    "    tasks = {'IE': {'yes': 0, 'no': 0, 'short': 0, 'long': 0},\n",
    "             'QA': {'yes': 0, 'no': 0, 'short': 0, 'long': 0},\n",
    "             'IR': {'yes': 0, 'no': 0, 'short': 0, 'long': 0},\n",
    "             'SUM': {'yes': 0, 'no': 0, 'short': 0, 'long': 0}}\n",
    "    with open(rte3_txt, \"r\") as rte3:\n",
    "        i = 1\n",
    "        for line in rte3:\n",
    "            # Process RTE-3 in chunks of 4 lines\n",
    "            if i == 1:\n",
    "                feats = line.split()\n",
    "                if feats[1] == split:\n",
    "                    task = feats[4]\n",
    "                    if feats[2] == \"YES\":\n",
    "                        tasks[task]['yes'] = tasks[task]['yes'] + 1\n",
    "                    else:\n",
    "                        tasks[task]['no'] = tasks[task]['no'] + 1\n",
    "                    if feats[5] == 'short':\n",
    "                        tasks[task]['short'] = tasks[task]['short'] + 1\n",
    "                    else:\n",
    "                        tasks[task]['long'] = tasks[task]['long'] + 1\n",
    "            elif i == 4:\n",
    "                i = 0\n",
    "            i += 1\n",
    "\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2203fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 388\n",
      "410 390\n"
     ]
    }
   ],
   "source": [
    "distribution = get_original_distribution('rte3.txt', 'dev')\n",
    "original_dev_yes = distribution['IE']['yes'] + distribution['QA']['yes'] + distribution['IR']['yes'] + distribution['SUM']['yes']\n",
    "original_dev_no = distribution['IE']['no'] + distribution['QA']['no'] + distribution['IR']['no'] + distribution['SUM']['no']\n",
    "print(original_dev_yes, original_dev_no)\n",
    "\n",
    "distribution = get_original_distribution('rte3.txt', 'test')\n",
    "original_test_yes = distribution['IE']['yes'] + distribution['QA']['yes'] + distribution['IR']['yes'] + distribution['SUM']['yes']\n",
    "original_test_no = distribution['IE']['no'] + distribution['QA']['no'] + distribution['IR']['no'] + distribution['SUM']['no']\n",
    "print(original_test_yes, original_test_no)\n",
    "\n",
    "orig_count_no = original_dev_no + original_test_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e082428",
   "metadata": {},
   "source": [
    "RTE-3 contains more cases where entailment holds (822) than cases where entailment does not hold (778). 11 entailment cases per tasks will be removed to balance the entailment labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7360e",
   "metadata": {},
   "source": [
    "## Converting the csv to a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbcb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def convert_to_dict(rte3_txt):\n",
    "    \"\"\"\n",
    "    Converts the translated RTE-3 txt to a Python dictionary.\n",
    "    \n",
    "    :param srte3_txt: path to the translated RTE-3 txt\n",
    "    :return: the RTE-3 dict\n",
    "    \"\"\"\n",
    "    \n",
    "    rte3 = defaultdict(dict)\n",
    "    # Build dict with all entries.\n",
    "    with open('rte3.txt', \"r\") as infile:\n",
    "        chunk_i = 1\n",
    "        for line in infile:\n",
    "            # Process RTE-3 in chunks of 4 lines\n",
    "            if chunk_i == 1:\n",
    "                feats = line.split()\n",
    "                # The key is artifically created.\n",
    "                if feats[0] not in rte3:\n",
    "                    id = feats[0]\n",
    "                else:\n",
    "                    id = int(feats[0]) + 1\n",
    "                rte3[id]['set'] = feats[1]\n",
    "                rte3[id]['entailment_label'] = feats[2]\n",
    "                rte3[id]['task'] = feats[4]\n",
    "                rte3[id]['length'] = feats[5]\n",
    "            elif chunk_i == 2:\n",
    "                rte3[id]['h'] = line.rstrip()\n",
    "            elif chunk_i == 3:\n",
    "                pass\n",
    "            else:  # chunk_i == 4\n",
    "                rte3[id]['t'] = line.rstrip()\n",
    "                chunk_i = 0\n",
    "            chunk_i += 1\n",
    "            \n",
    "    return rte3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205a35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rte3 = convert_to_dict('rte3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260a546",
   "metadata": {},
   "source": [
    "## Randomising the dataset to randomly remove sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7424fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_rte3(rte3_dict):\n",
    "    \"\"\"\n",
    "    Shuffles the RTE-3 dict so that sentence pairs can randomly be skipped to balance the dataset.\n",
    "    \n",
    "    :param rte3_dict: dict of RTE-3\n",
    "    :return: the shuffled RTE-3 dict\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = list(rte3_dict.keys())\n",
    "    random.Random(1).shuffle(keys)\n",
    "    return {key: rte3_dict[key] for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b8cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rte3_shuffled = shuffle_rte3(rte3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc4abc6",
   "metadata": {},
   "source": [
    "## Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634ee979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(rte3_shuffled):\n",
    "    \"\"\"\n",
    "    Removes 11 cases where entailments holds for each NLP task.\n",
    "    \n",
    "    :param rte3: the shuffled RTE-3 dict\n",
    "    :return rte3: balanced RTE-3 dict\n",
    "    \"\"\"\n",
    "    ie = qa = ir = sum = 11\n",
    "    keys = list(rte3_shuffled.keys())\n",
    "    random.Random(1).shuffle(keys)\n",
    "    rte3_shuffled = {key: rte3[key] for key in keys}\n",
    "    for pair_id, feats in dict(rte3_shuffled).items():\n",
    "        if feats['task'] == \"IE\" and feats['entailment_label'] == \"YES\" and ie != 0:\n",
    "            del rte3_shuffled[pair_id]\n",
    "            ie -= 1\n",
    "        elif feats['task'] == \"QA\" and feats['entailment_label'] == \"YES\" and qa != 0:\n",
    "            del rte3_shuffled[pair_id]\n",
    "            qa -= 1\n",
    "        elif feats['task'] == \"IR\" and feats['entailment_label'] == \"YES\" and ir != 0:\n",
    "            del rte3_shuffled[pair_id]\n",
    "            ir -= 1\n",
    "        elif feats['task'] == \"SUM\" and feats['entailment_label'] == \"YES\" and sum != 0:\n",
    "            del rte3_shuffled[pair_id]\n",
    "            sum -= 1\n",
    "            \n",
    "    return rte3_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a09e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "rte3_shuffled = balance_dataset(rte3_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94581fc",
   "metadata": {},
   "source": [
    "## Splitting the data into 3 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c105a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "def make_splits(dev_split, train_split):\n",
    "    \"\"\"\n",
    "    Divides the RTE-3 dict into two development sets and a test set where \n",
    "    each split consists of a 50-50 ratio between positive and negative pairs.\n",
    "    \n",
    "    :param dev_split: proportion of data to be assigned to the dev set\n",
    "    :param train_split: proportion of data to be assigned to the train set\n",
    "    :return: the RTE-3 dev, train and test split\n",
    "    \"\"\"\n",
    "    \n",
    "    test_count_no = dev_count_yes = train_count_yes = dev_count_no = train_count_no = 0\n",
    "    dev = defaultdict(dict)\n",
    "    train = defaultdict(dict)\n",
    "    test = defaultdict(dict)\n",
    "    for pair_id, feats in rte3_shuffled.items():\n",
    "        if feats['entailment_label'] == \"YES\":\n",
    "            if dev_count_yes < floor(orig_count_no / 100 * dev_split):\n",
    "                dev[pair_id] = feats\n",
    "                dev_count_yes += 1\n",
    "            elif train_count_yes < ceil(orig_count_no / 100 * train_split):\n",
    "                train[pair_id] = feats\n",
    "                train_count_yes += 1\n",
    "            else:\n",
    "                test[pair_id] = feats\n",
    "        else:  # NO\n",
    "            if dev_count_no < floor(orig_count_no / 100 * dev_split):\n",
    "                dev[pair_id] = feats\n",
    "                dev_count_no += 1\n",
    "            elif train_count_no < ceil(orig_count_no / 100 * train_split):\n",
    "                train[pair_id] = feats\n",
    "                train_count_no += 1\n",
    "            else:\n",
    "                test[pair_id] = feats\n",
    "                \n",
    "    return dev, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9893d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev, train, test = make_splits(60, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f0734",
   "metadata": {},
   "source": [
    "## Verifying the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a830143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_distribution(rte3_split):\n",
    "    \"\"\"\n",
    "    Retrieves the distribution of RTE-3 dicts.\n",
    "\n",
    "    :param rte3_split: path to a RTE-3 dict split\n",
    "    :return: dict denoting the distribution of the entailment and length label divided over the four NLP tasks\n",
    "    \"\"\"\n",
    "    tasks = {'IE': {'yes': 0, 'no': 0, 'short': 0, 'long': 0},\n",
    "             'QA': {'yes': 0, 'no': 0, 'short': 0, 'long': 0},\n",
    "             'IR': {'yes': 0, 'no': 0, 'short': 0, 'long': 0},\n",
    "             'SUM': {'yes': 0, 'no': 0, 'short': 0, 'long': 0}}\n",
    " \n",
    "    for feats in rte3_split.values():\n",
    "        if feats['entailment_label'] == 'YES':\n",
    "            tasks[feats['task']]['yes'] = tasks[feats['task']]['yes'] + 1\n",
    "        else:\n",
    "            tasks[feats['task']]['no'] = tasks[feats['task']]['no'] + 1\n",
    "        if feats['length'] == 'short':\n",
    "            tasks[feats['task']]['short'] = tasks[feats['task']]['short'] + 1\n",
    "        else:\n",
    "            tasks[feats['task']]['long'] = tasks[feats['task']]['long'] + 1\n",
    "\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1263c0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev: 466 466\n",
      "train: 156 156\n",
      "test: 156 156\n"
     ]
    }
   ],
   "source": [
    "distribution = get_new_distribution(dev)\n",
    "yes = distribution['IE']['yes'] + distribution['QA']['yes'] + distribution['IR']['yes'] + distribution['SUM']['yes']\n",
    "no = distribution['IE']['no'] + distribution['QA']['no'] + distribution['IR']['no'] + distribution['SUM']['no']\n",
    "print('dev:', yes, no)\n",
    "\n",
    "distribution = get_new_distribution(train)\n",
    "yes = distribution['IE']['yes'] + distribution['QA']['yes'] + distribution['IR']['yes'] + distribution['SUM']['yes']\n",
    "no = distribution['IE']['no'] + distribution['QA']['no'] + distribution['IR']['no'] + distribution['SUM']['no']\n",
    "print('train:', yes, no)\n",
    "\n",
    "distribution = get_new_distribution(test)\n",
    "yes = distribution['IE']['yes'] + distribution['QA']['yes'] + distribution['IR']['yes'] + distribution['SUM']['yes']\n",
    "no = distribution['IE']['no'] + distribution['QA']['no'] + distribution['IR']['no'] + distribution['SUM']['no']\n",
    "print('test:', yes, no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec9a72",
   "metadata": {},
   "source": [
    "## Getting average sentence length (in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d71ac4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average t_s length: 28.6 tokens\n",
      "Average t_l length: 56.1 tokens\n",
      "Average h length: 8.4 tokens\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('nl_core_news_lg')\n",
    "token_c_t_s = token_c_t_l = token_c_h = pair_c = text_s_c = text_l_c = 0\n",
    "for entry in {**dev, **train, **test}.values():\n",
    "    if entry['length'] == 'short':\n",
    "        token_c_t_s += len([token.text for sent in nlp(entry['t']).sents for token in sent if not token.is_punct])\n",
    "        text_s_c += 1\n",
    "    else:  # long\n",
    "        token_c_t_l += len([token.text for sent in nlp(entry['t']).sents for token in sent if not token.is_punct])\n",
    "        text_l_c += 1\n",
    "    token_c_h += len([token.text for sent in nlp(entry['h']).sents for token in sent if not token.is_punct])\n",
    "    pair_c += 1\n",
    "    \n",
    "print(\"Average t_s length:\", round(token_c_t_s / text_s_c, 1), \"tokens\")\n",
    "print(\"Average t_l length:\", round(token_c_t_l / text_l_c, 1), \"tokens\")\n",
    "print(\"Average h length:\", round(token_c_h / pair_c, 1), \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415958b4",
   "metadata": {},
   "source": [
    "## Serialising the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dfa0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(dict(dev), open(\"dev.p\", \"wb\"))\n",
    "pickle.dump(dict(train), open(\"train.p\", \"wb\"))\n",
    "pickle.dump(dict(test), open(\"test.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
